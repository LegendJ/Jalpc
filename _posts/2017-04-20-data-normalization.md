---
layout: post
title: "数据标准化(normalization)"
data: 2017-04-20
categories: [Ml]
tags: [normalization,SGD,regression]
icon: icon-html
---


# 为什么做数据标准化（作用）
> 标准化: 把需要处理的数据经过处理后（按比例缩放等），限制在一个特定区间内。
> 归一化狭义的理解为把数据调整在[0,1]或[-1,1]的区间内，也是一种normalization;标准化(standardization)一般处理成均值0-方差1。
- 无量纲化：例如房子数量和收入，因为从业务层知道，这两者的重要性一样，所以把它们全部归一化。 这是从业务层面上作的处理。
- 模型求解需要:如梯度下降法不归一化，容易产生陕谷，而学习率较大时，以之字形下降。学习率较小，则会产生直角形路线，不管怎么样，都不会是好路线。且归一化后，加快了收敛的速度。

<img src="{{ site.img_path }}/normal/sgd.png" width="75%">
椭圆代表损失函数的等值线，两个坐标轴代表两个特征。
<img src="{{ site.img_path }}/normal/sgd1.png" width="50%">
<img src="{{ site.img_path }}/normal/sgd2.png" width="50%">

因此在使用梯度下降法求解时，归一化很有必要。

- 提高精度：一些分类器需要计算样本之间的距离（如欧氏距离），例如KNN、K-means，防止单一属性Dominating。如果一个特征值域范围非常大，那么距离计算就主要取决于这个特征，从而与实际情况相悖（比如这时实际情况是值域范围小的特征更重要）,i.e.使特征具有相同的Scale。
- note:不同问题有不同的意义,PCA,BP神经网络(正在学习中),SVM,LinearRegression。

# 怎么做数据标准化（方法）


- 特征缩放（min-max标准化）: 也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下:
  $$
  x=x-min/max-min
  $$
  　　其中max为样本数据的最大值，min为样本数据的最小值。这种方法有一个缺陷就是当有新数据加入时，可能导致max和min的变化，需要重新定义，实际使用中可以用经验常量值来替代max和min。

- Z-score 0均值标准化:经过处理的数据符合标准正态分布，即均值为0，标准差为1，其转化函数为:
  $$
  x = (x - u)/σ \\u: 所有样本数据的均值,σ: 为所有样本数据的标准差
  $$

  在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，第二种方法(Z-score standardization)表现更好。

- 非线性归一化log,arctan函数

  - 用反正切函数也可以实现数据的归一化：使用这个方法需要注意的是如果想映射的区间为[0,1]，则数据都应该大于等于0，小于0的数据将被映射到[-1,0]区间上。

- log transform:在归一化的场景来说，就是将数据分化较大的分布，变得相对小一些，但并不像min-max一样缩放在[0,1]区间内。

  log转换在回归问题中还有别的作用，所以独立开来简单说下，为了满足回归模型假设（误差项零均值、同方差且互不相关）常对因变量或者对有偏度（正偏态分布）的变量做log转换（看到kaggle上有人这么做，暂不理解为什么对自变量做log转换，还是说对特征的分布也有要求？），消除分布的异方差性，使偏态分布转换为近似正态分布，注意一点，预处理怎么变过来，预测时逆变换转回去。

# 什么时候做数据标准化

- 变量是不同的物理量，数值的量纲不同 

- 没有“外部”知识告诉我们，变量值或者方差越大就越重要对各维分别做归一化会丢失各维方差这一信息，但各维之间的相关系数可以保留。如果本来各维的量纲是相同的，最好不要做归一化，以尽可能多地保留信息。如果本来各维的量纲是不同的，那么直接做PCA没有意义，就需要先对各维分别归一化。

- #### Note:看模型是否伸缩不变 。PS.总的来说，用梯度下降求解的（LR,SVMS,neural network etc.）最好做标准化，涉及欧氏距离的做标准化，LDA,PCA这些与求解时与**方差**最大化有关系的做标准化。理解模型！！！

  有些模型在各个维度进行不均匀伸缩后，最优解与原来不等价，例如SVM。对于这样的模型，除非本来各维数据的分布范围就比较接近，否则**必须**进行标准化，以免模型参数被分布范围较大或较小的数据dominate。

  有些模型在各个维度进行不均匀伸缩后，最优解与原来等价，例如logistic regression。对于这样的模型，是否标准化理论上不会改变最优解。但是，由于实际求解往往使用迭代算法，如果目标函数的形状太“扁”，迭代算法可能收敛得很慢甚至不收敛。所以对于具有伸缩不变性的模型，**最好**也进行数据标准化。

  - Q: 什么是伸缩不变性(scale invariant)?
  - A: 看数据经过变换后，与原来的最优解等价的模型是否仍是最优解
  - Q: svm和LR为什么有不同的性质?
  - A: 损失函数的不同导致了可伸缩性的不同，LR的lossfunction: $l(\beta) = \sum(-y_i\beta^Tx_i+ln(1+e^{\beta^Tx_i}))$ ,对Xi进行缩放，实际在i维上$\beta^Tx $的值没有发生改变，(见下面多元线性回归的解释)，而对于svm，求解支持向量时，涉及到欧式距离的求解，缩放前后超平面不同，故而不具有伸缩不变性。
  - 通过下面的推导可以得出，将x1放大a倍等价于将参数缩小a倍，然而如果进行log transform或者加入了正则化项，前后解就不一样了。例如岭回归，L2会对参数和进行限制，这时候做normalization的意思就是让每个参数被同等约束（雨露均沾233）

  $$
  \hat{\beta_1}(x_1)=\sum_{i=1}^{n}(x_{1,i}-\overline{x}_1)(y_i-\overline{y})/\sum_{i=1}^{n}(x_{1,i}-\overline{x}_1)^2
  $$



$$
=>\hat{\beta_1}(ax_1)=\frac{\sum_{i=1}^{n}(ax_{1,i}-a\overline{x}_1)(y_i-\overline{y})}{\sum_{i=1}^{n}(ax_{1,i}-a\overline{x}_1)^2}=\hat{\beta_1}(ax_1)/a
$$
- 而且缩放前后对于regressor（回归系数）来说，只改变了$\beta_1$,而不改变其他系数。


##### 参考资料

- https://www.zhihu.com/question/30038463/answer/30038463来源：知乎,作者：王赟 Maigo

